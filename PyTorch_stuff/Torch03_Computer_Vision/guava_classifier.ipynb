{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchmetrics\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys \n",
    "import os\n",
    "from pathlib import Path\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "from torch_device import DEVICE\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Directory and Subdirectory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder PATH listing for volume Data\n",
      "Volume serial number is 3209-780B\n",
      "D:.\n",
      "\\---GuavaDiseaseDataset\n",
      "    +---test\n",
      "    |   +---Anthracnose\n",
      "    |   +---fruit_fly\n",
      "    |   \\---healthy_guava\n",
      "    +---train\n",
      "    |   +---Anthracnose\n",
      "    |   +---fruit_fly\n",
      "    |   \\---healthy_guava\n",
      "    \\---val\n",
      "        +---Anthracnose\n",
      "        +---fruit_fly\n",
      "        \\---healthy_guava\n"
     ]
    }
   ],
   "source": [
    "!tree /A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"GuavaDiseaseDataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 directories and 1 images in 'GuavaDiseaseDataset'.\n",
      "There are 0 directories and 156 images in 'GuavaDiseaseDataset\\test\\Anthracnose'.\n",
      "There are 0 directories and 132 images in 'GuavaDiseaseDataset\\test\\fruit_fly'.\n",
      "There are 0 directories and 94 images in 'GuavaDiseaseDataset\\test\\healthy_guava'.\n",
      "There are 0 directories and 1080 images in 'GuavaDiseaseDataset\\train\\Anthracnose'.\n",
      "There are 0 directories and 918 images in 'GuavaDiseaseDataset\\train\\fruit_fly'.\n",
      "There are 0 directories and 649 images in 'GuavaDiseaseDataset\\train\\healthy_guava'.\n",
      "There are 0 directories and 308 images in 'GuavaDiseaseDataset\\val\\Anthracnose'.\n",
      "There are 0 directories and 262 images in 'GuavaDiseaseDataset\\val\\fruit_fly'.\n",
      "There are 0 directories and 185 images in 'GuavaDiseaseDataset\\val\\healthy_guava'.\n"
     ]
    }
   ],
   "source": [
    "dir_path = data_path\n",
    "for dirpath, dirnames, filenames in os.walk(dir_path):\n",
    "    if len(filenames) > 0:\n",
    "        print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & Test Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('GuavaDiseaseDataset/train'),\n",
       " WindowsPath('GuavaDiseaseDataset/test'))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = data_path / \"train\"\n",
    "test_dir = data_path / \"test\"\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Function To Display An Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DisplayImage(img : Image, title : str = None):\n",
    "    plt.imshow(img)\n",
    "    if title != None:\n",
    "        plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Train & Test Images Into A DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Compose(\n",
       "     Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=True)\n",
       "     RandomHorizontalFlip(p=0.5)\n",
       "     ToTensor()\n",
       " ),\n",
       " Compose(\n",
       "     Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=True)\n",
       "     ToTensor()\n",
       " ))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    # transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    # transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_transform, test_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=train_transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Anthracnose', 'fruit_fly', 'healthy_guava'],\n",
       " {'Anthracnose': 0, 'fruit_fly': 1, 'healthy_guava': 2})"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = train_dataset.classes\n",
    "class_dict = train_dataset.class_to_idx\n",
    "classes, class_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Setup (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x18eddf6d3f0>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Guava_CNN(\n",
       "   (block1): Sequential(\n",
       "     (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (1): ReLU()\n",
       "     (2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (4): ReLU()\n",
       "     (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "   )\n",
       "   (block2): Sequential(\n",
       "     (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (1): ReLU()\n",
       "     (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "   )\n",
       "   (classifier): Sequential(\n",
       "     (0): Flatten(start_dim=1, end_dim=-1)\n",
       "     (1): Linear(in_features=40960, out_features=128, bias=True)\n",
       "     (2): ReLU()\n",
       "     (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "     (4): ReLU()\n",
       "     (5): Linear(in_features=64, out_features=32, bias=True)\n",
       "     (6): Linear(in_features=32, out_features=3, bias=True)\n",
       "   )\n",
       " ),\n",
       " 'total_parameteres=5255563')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Guava_CNN(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_units, output_shape, conv_k_size = 3, conv_stride = 1, pool_k_size = 2, pool_stride =2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, out_channels=hidden_units, kernel_size=conv_k_size, stride=conv_stride, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(num_features=hidden_units),\n",
    "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=conv_k_size, stride=conv_stride, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_k_size, stride=pool_stride)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=conv_k_size, stride=conv_stride, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=conv_k_size, stride=conv_stride, padding=1),\n",
    "            # nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_k_size, stride=pool_stride)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units * 64*64 # Change!!!\n",
    "                      ,out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=32),\n",
    "            nn.Linear(in_features=32, out_features=3)\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_b1 = self.block1(x)\n",
    "        # print(f\"--------\\n{y_b1.shape=}\")\n",
    "        y_b2 = self.block2(y_b1)\n",
    "        # print(f\"{y_b2.shape=}\")\n",
    "        y = self.classifier(y_b2)\n",
    "        # print(f\"{y.shape=}\\n--------\")\n",
    "        return y\n",
    "\n",
    "cnn = Guava_CNN(input_shape=3, hidden_units=10, output_shape=len(classes)).to(DEVICE)\n",
    "total_parameteres = sum(p.numel() for p in cnn.parameters())\n",
    "cnn, f\"{total_parameteres=}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, (X,y) in enumerate(train_dataloader):\n",
    "    X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "    y_pred = cnn.forward(X)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function (CEL) and Optimizer (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_cnn = nn.CrossEntropyLoss()\n",
    "optimizer_cnn = torch.optim.Adam(params=cnn.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & Test Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model : nn.Module , dataloader: torch.utils.data.DataLoader,\n",
    "            loss_func : nn.Module, optimizer : torch.optim.Optimizer, num_classes,\n",
    "            accuracy_func = torchmetrics.functional.accuracy, device = DEVICE):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    model.to(device=device)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        y_pred = model.forward(X)\n",
    "\n",
    "        batch_loss = loss_func(y_pred, y)\n",
    "        batch_acc = accuracy_func(preds=y_pred, target=y,task='multiclass', num_classes=num_classes)\n",
    "        if batch % (int(len(dataloader) / 10)) == 0:\n",
    "            print(f\"\\tBatch {batch} | Loss: {batch_loss} | Accuracy: {batch_acc}\")\n",
    "\n",
    "        train_loss += batch_loss \n",
    "        train_acc += batch_acc\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc /= len(dataloader)\n",
    "\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}\")\n",
    "    \n",
    "def test_step(model: nn.Module, dataloader : torch.utils.data.DataLoader, \n",
    "              loss_func: nn.Module, num_classes, accuracy_func: torchmetrics.functional.accuracy, device = DEVICE):\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model.to(device=DEVICE)\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            y_pred = model.forward(X)\n",
    "\n",
    "            test_loss += loss_func(y_pred, y)\n",
    "            test_acc += accuracy_func(preds=y_pred, target=y,task='multiclass', num_classes=num_classes)\n",
    "\n",
    "        test_loss /= len(dataloader)\n",
    "        test_acc /= len(dataloader)\n",
    "\n",
    "        print(f\"Test loss: {test_loss:.5f} | Test   accuracy: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training & Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "\tBatch 0 | Loss: 1.1211286783218384 | Accuracy: 0.28125\n",
      "\tBatch 8 | Loss: 0.43153515458106995 | Accuracy: 0.8125\n",
      "\tBatch 16 | Loss: 0.47175294160842896 | Accuracy: 0.78125\n",
      "\tBatch 24 | Loss: 0.5257145762443542 | Accuracy: 0.78125\n",
      "\tBatch 32 | Loss: 0.48481959104537964 | Accuracy: 0.78125\n",
      "\tBatch 40 | Loss: 0.1851448118686676 | Accuracy: 0.90625\n",
      "\tBatch 48 | Loss: 0.6364563703536987 | Accuracy: 0.8125\n",
      "\tBatch 56 | Loss: 0.09008537977933884 | Accuracy: 1.0\n",
      "\tBatch 64 | Loss: 0.2955690026283264 | Accuracy: 0.8125\n",
      "\tBatch 72 | Loss: 0.29318535327911377 | Accuracy: 0.90625\n",
      "\tBatch 80 | Loss: 0.20058035850524902 | Accuracy: 0.90625\n",
      "Train loss: 0.41840 | Train accuracy: 0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:24<01:37, 24.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.34268 | Test   accuracy: 0.88\n",
      "Epoch 1:\n",
      "\tBatch 0 | Loss: 0.2379794865846634 | Accuracy: 0.9375\n",
      "\tBatch 8 | Loss: 0.1517874300479889 | Accuracy: 0.96875\n",
      "\tBatch 16 | Loss: 0.38639748096466064 | Accuracy: 0.84375\n",
      "\tBatch 24 | Loss: 0.17329426109790802 | Accuracy: 0.96875\n",
      "\tBatch 32 | Loss: 0.4456913471221924 | Accuracy: 0.875\n",
      "\tBatch 40 | Loss: 0.4044700264930725 | Accuracy: 0.8125\n",
      "\tBatch 48 | Loss: 0.38312986493110657 | Accuracy: 0.8125\n",
      "\tBatch 56 | Loss: 0.493373841047287 | Accuracy: 0.84375\n",
      "\tBatch 64 | Loss: 0.11528097093105316 | Accuracy: 0.96875\n",
      "\tBatch 72 | Loss: 0.22918367385864258 | Accuracy: 0.875\n",
      "\tBatch 80 | Loss: 0.19230718910694122 | Accuracy: 0.90625\n",
      "Train loss: 0.25819 | Train accuracy: 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:49<01:13, 24.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.24352 | Test   accuracy: 0.91\n",
      "Epoch 2:\n",
      "\tBatch 0 | Loss: 0.06823782622814178 | Accuracy: 1.0\n",
      "\tBatch 8 | Loss: 0.09553790837526321 | Accuracy: 0.96875\n",
      "\tBatch 16 | Loss: 0.3530217409133911 | Accuracy: 0.875\n",
      "\tBatch 24 | Loss: 0.18327775597572327 | Accuracy: 0.9375\n",
      "\tBatch 32 | Loss: 0.17086823284626007 | Accuracy: 0.90625\n",
      "\tBatch 40 | Loss: 0.18486663699150085 | Accuracy: 0.96875\n",
      "\tBatch 48 | Loss: 0.12216278165578842 | Accuracy: 0.9375\n",
      "\tBatch 56 | Loss: 0.16119451820850372 | Accuracy: 0.9375\n",
      "\tBatch 64 | Loss: 0.30955764651298523 | Accuracy: 0.875\n",
      "\tBatch 72 | Loss: 0.2625788450241089 | Accuracy: 0.875\n",
      "\tBatch 80 | Loss: 0.15167368948459625 | Accuracy: 0.96875\n",
      "Train loss: 0.19026 | Train accuracy: 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [01:13<00:49, 24.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.20932 | Test   accuracy: 0.91\n",
      "Epoch 3:\n",
      "\tBatch 0 | Loss: 0.09649941325187683 | Accuracy: 0.96875\n",
      "\tBatch 8 | Loss: 0.22563593089580536 | Accuracy: 0.875\n",
      "\tBatch 16 | Loss: 0.03310680761933327 | Accuracy: 1.0\n",
      "\tBatch 24 | Loss: 0.3322550058364868 | Accuracy: 0.875\n",
      "\tBatch 32 | Loss: 0.15670832991600037 | Accuracy: 0.9375\n",
      "\tBatch 40 | Loss: 0.09246756136417389 | Accuracy: 0.96875\n",
      "\tBatch 48 | Loss: 0.1990651935338974 | Accuracy: 0.9375\n",
      "\tBatch 56 | Loss: 0.2957402467727661 | Accuracy: 0.84375\n",
      "\tBatch 64 | Loss: 0.16950276494026184 | Accuracy: 0.90625\n",
      "\tBatch 72 | Loss: 0.13132794201374054 | Accuracy: 0.90625\n",
      "\tBatch 80 | Loss: 0.20033875107765198 | Accuracy: 0.96875\n",
      "Train loss: 0.15468 | Train accuracy: 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [01:37<00:24, 24.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.14340 | Test   accuracy: 0.95\n",
      "Epoch 4:\n",
      "\tBatch 0 | Loss: 0.20389433205127716 | Accuracy: 0.90625\n",
      "\tBatch 8 | Loss: 0.20085987448692322 | Accuracy: 0.96875\n",
      "\tBatch 16 | Loss: 0.2985471487045288 | Accuracy: 0.875\n",
      "\tBatch 24 | Loss: 0.05439038202166557 | Accuracy: 0.96875\n",
      "\tBatch 32 | Loss: 0.14723806083202362 | Accuracy: 0.9375\n",
      "\tBatch 40 | Loss: 0.11825314164161682 | Accuracy: 0.96875\n",
      "\tBatch 48 | Loss: 0.168707475066185 | Accuracy: 0.9375\n",
      "\tBatch 56 | Loss: 0.08076707273721695 | Accuracy: 0.96875\n",
      "\tBatch 64 | Loss: 0.040293581783771515 | Accuracy: 1.0\n",
      "\tBatch 72 | Loss: 0.02844599261879921 | Accuracy: 1.0\n",
      "\tBatch 80 | Loss: 0.2086792141199112 | Accuracy: 0.9375\n",
      "Train loss: 0.11863 | Train accuracy: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:02<00:00, 24.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.23543 | Test   accuracy: 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch {epoch}:\")\n",
    "    train_step(cnn, train_dataloader, loss_fn_cnn, optimizer_cnn, len(classes), device=DEVICE)\n",
    "    test_step(cnn, test_dataloader, loss_fn_cnn, len(classes), torchmetrics.functional.accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict':cnn.state_dict(),\n",
    "    'optimizer_state_dict':optimizer_cnn.state_dict()\n",
    "}, 'guava_classifer_cnn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Guava_CNN(\n",
       "  (block1): Sequential(\n",
       "    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=40960, out_features=128, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (6): Linear(in_features=32, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Guava_CNN(3, 10, 3)\n",
    "model.load_state_dict(torch.load('guava_classifer_cnn.pth', weights_only=False)['model_state_dict'])\n",
    "optimizer_cnn = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "optimizer_cnn.load_state_dict(torch.load('guava_classifer_cnn.pth', weights_only=False)['optimizer_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_stuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
